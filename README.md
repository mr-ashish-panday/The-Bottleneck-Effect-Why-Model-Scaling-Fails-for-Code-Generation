# Paper 11: Why Code Generation Actually Fails

**Execution-Aware Analysis of Code Generation Model Failures**

---

## ğŸ¯ Project Overview

This repository contains research code for studying **why code generation models fail** through execution-aware analysis. Unlike prior work that treats code generation as a text generation problem, we analyze failures through the lens of **execution semantics**.

### Research Questions
1. How do code generation failures differ from NLP failures?
2. Can we predict failure types from model internals?
3. Can execution-aware decoding reduce crashes without retraining?

---

## ğŸ“ Project Structure

```
paper11_code_execution_failures/
â”œâ”€â”€ data/Â  Â  Â  Â  Â  Â  Â  Â  Â # Datasets and results
â”œâ”€â”€ models/Â  Â  Â  Â  Â  Â  Â  Â # Model checkpoints and configs
â”œâ”€â”€ src/Â  Â  Â  Â  Â  Â  Â  Â  Â  # Source code
â”‚Â  Â â”œâ”€â”€ data/Â  Â  Â  Â  Â  Â  # Data loading
â”‚Â  Â â”œâ”€â”€ models/Â  Â  Â  Â  Â  # Model wrappers
â”‚Â  Â â”œâ”€â”€ evaluation/Â  Â  Â  # Execution engine
â”‚Â  Â â””â”€â”€ analysis/Â  Â  Â  Â  # Failure analysis
â”œâ”€â”€ scripts/Â  Â  Â  Â  Â  Â  Â  # Executable scripts
â”œâ”€â”€ notebooks/Â  Â  Â  Â  Â  Â  # Jupyter notebooks
â”œâ”€â”€ outputs/Â  Â  Â  Â  Â  Â  Â  # Figures, tables, logs
â””â”€â”€ tests/Â  Â  Â  Â  Â  Â  Â  Â  # Unit tests
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
### 1. Environment Setup

```bash
# Clone repository
git clone [https://github.com/yourusername/LLM-Bottleneck-Effect.git](https://github.com/yourusername/LLM-Bottleneck-Effect.git)
cd LLM-Bottleneck-Effect

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
```

### 2. Download Data

python scripts/download_data.py --dataset all

---

ğŸ§ª Reproduction

To replicate the results from the paper, follow these steps:

Step 1: Run the Generation Loop (The "16k Experiments")
This script generates solutions and immediately executes them to label "Success" vs "Failure".

To replicate the results from the paper, follow these steps:

Step 1: Run the Generation Loop (The "16k Experiments")
This script generates solutions and immediately executes them to label "Success" vs "Failure".



---

## ğŸ”§ Configuration

Edit `config.yaml` to customize:
- Model selection (`gpt2`, `gpt2-medium`)
- Generation parameters (temperature, top_p)
- Hardware constraints (GPU memory)
- Failure categories

---

## ğŸ“ˆ Experiment Tracking

```bash
# Optional: Use Weights & Biases
pip install wandb
wandb login

# Enable in config.yaml
tracking:
Â  use_wandb: true
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ“ Citation

If you use this code, please cite:

```bibtex
@article{pandey2025bottleneck,
  title={The Bottleneck Effect: Why Model Scaling Fails for Code Generation},
  author={Pandey, Ashish},
  journal={arXiv preprint},
  year={2025}
}
```

---

## ğŸ“§ Contact

- **Author**: Ashish Pandey
- **Email**: ashishpandey9818@gmail.com
- **Institution**: Khwopa College Of Engineering

---

## ğŸ”’ License

MIT License - see LICENSE file for details

---

## âš ï¸ Hardware Requirements

- **GPU**: 8-12 GB VRAM (tested on RTX 3060/4070 Ti)
- **RAM**: 16 GB minimum
- **Storage**: 50 GB for data + checkpoints
- **Time**: ~95 GPU hours total (spread over 8 weeks)

---
